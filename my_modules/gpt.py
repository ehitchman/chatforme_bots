import os
import requests
import openai
import tiktoken
from typing import List
import re
import copy

from my_modules import utils
from my_modules.my_logging import create_logger
from my_modules.config import run_config

#LOGGING
stream_logs = True
runtime_logger_level = 'WARNING'
yaml_data = run_config()
gpt_model = yaml_data['openai-api']['assistant_model']
shorten_message_prompt = yaml_data['ouat_prompts']['shorten_response_length_prompt']

logger = create_logger(
    dirname='log',
    logger_name='logger_gpt',
    debug_level=runtime_logger_level,
    mode='w',
    stream_logs=stream_logs
    )

def create_gpt_client():
    client = openai.OpenAI(api_key = os.getenv('OPENAI_API_KEY'))
    return client

# call to chat gpt for completion TODO: Could add  limits here?
def openai_gpt_chatcompletion(
        messages_dict_gpt:list[dict],
        max_characters=200,
        max_attempts=5,
        model=gpt_model,
        frequency_penalty=1.1,
        presence_penalty=1.1,
        temperature=0.6
        ) -> str: 
    """
    Send a message to OpenAI GPT-3.5-turbo for completion and get the response.

    Parameters:
    - messages_dict_gpt (dict): Dictionary containing the message structure for the GPT prompt.
    - OPENAI_API_KEY (str): API key to authenticate with OpenAI.

    Returns:
    str: The content of the message generated by GPT.
    """  
    #Create Client
    client = create_gpt_client()  

    logger.debug("This is the messages_dict_gpt submitted to GPT ChatCompletion")
    logger.debug(f"The number of tokens included is: {_count_tokens_in_messages(messages=messages_dict_gpt)}")
    logger.debug(messages_dict_gpt)

    #Error checking for token length, etc.
    counter=0
    try:
        while _count_tokens_in_messages(messages=messages_dict_gpt) > 2000:
            if counter > 5:
                error_message = f"Error: Too many tokens {token_count} even after 3 attempts to reduce count"
                logger.error(error_message)
                raise ValueError(error_message)
            logger.debug("Entered _count_tokens_in_messages() > ____")
            token_count = _count_tokens_in_messages(messages=messages_dict_gpt)
            logger.warning(f"The messages_dict_gpt contained too many tokens {(token_count)}, .pop(0) first dict")
            messages_dict_gpt.pop(0)
            counter+=1
    except Exception as e:
        logger.error(f"Exception ocurred in openai_gpt_chatcompletion() during _count_tokens_in_messages(): {e}")
    
    logger.info(f"messages_dict_gpt submitted to GPT ChatCompletion (tokens: {_count_tokens_in_messages(messages=messages_dict_gpt)})")
    logger.debug(messages_dict_gpt)

    #Call to OpenAI #TODO: This loop is wonky.  Should probably divert to a 'while' statement
    for attempt in range(max_attempts):
        logger.debug(f"THIS IS ATTEMPT #{attempt + 1}")
        try:
            generated_response = client.chat.completions.create(
                model=model,
                messages=messages_dict_gpt,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                temperature=temperature
            )
        except Exception as e:
            logger.error(f"Exception occurred during API call: {e}: Attempt {attempt + 1} of {max_attempts} failed.")
            continue

        logger.debug(f"Completed generated response using client.chat.completions.create")          
        gpt_response_text = generated_response.choices[0].message.content
        gpt_response_text_len = len(gpt_response_text)
  
        logger.debug(f"generated_response type: {type(generated_response)}, length: {gpt_response_text_len}:")

        if gpt_response_text_len < max_characters:
            logger.info(f'OK: The generated message was <{max_characters} characters')
            break  

        else: # Did not get a msg < n chars, try again.
            logger.warning(f'\gpt_response_text_len: >{max_characters} characters, retrying call to openai_gpt_chatcompletion')
            messages_dict_gpt_updated = [{'role':'user', 'content':f"{shorten_message_prompt}: '{gpt_response_text}'"}]
            generated_response = client.chat.completions.create(
                model=model,
                messages=messages_dict_gpt_updated,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                temperature=temperature
                )
            gpt_response_text = generated_response.choices[0].message.content
            gpt_response_text_len = len(gpt_response_text)

            if gpt_response_text_len > max_characters:
                logger.warning(f'gpt_response_text length was {gpt_response_text_len} characters (max: {max_characters}), trying again...')
            elif gpt_response_text_len < max_characters:
                logger.info(f"OK on attempt --{attempt}-- gpt_response_text: {gpt_response_text}")
                break
    else:
        message = "Maxium GPT call retries exceeded"
        logger.error(message)        
        raise Exception(message)

    return gpt_response_text

def prompt_text_replacement(gpt_prompt_text,
                            replacements_dict):
    prompt_text_replaced = gpt_prompt_text.format(**replacements_dict)   
    logger.debug(f"prompt_text_replaced: {prompt_text_replaced}")
    return prompt_text_replaced

def ouat_gpt_response_cleanse(gpt_response: str) -> str:
    gpt_response_formatted = re.sub(r'<<<.*?>>>\s*:', '', gpt_response)
    return gpt_response_formatted

def chatforme_gpt_response_cleanse(gpt_response: str) -> str:
    gpt_response_formatted = re.sub(r'<<<.*?>>>\s*:', '', gpt_response)
    return gpt_response_formatted

def botthot_gpt_response_cleanse(gpt_response: str) -> str:
    gpt_response_formatted = re.sub(r'<<<.*?>>>\s*:', '', gpt_response)
    return gpt_response_formatted

def combine_msghistory_and_prompttext(prompt_text,
                                      prompt_text_role='user',
                                      prompt_text_name='unknown',
                                      msg_history_list_dict=None,
                                      combine_messages=False,
                                      output_new_list=False) -> [dict]:
    
    if output_new_list == True:
        msg_history_list_dict_temp = copy.deepcopy(msg_history_list_dict)
    else:
        msg_history_list_dict_temp = msg_history_list_dict

    if prompt_text_role == 'system':
        prompt_dict = {'role': prompt_text_role, 'content': f'{prompt_text}'}
    elif prompt_text_role in ['user', 'assistant']:
        prompt_dict = {'role': prompt_text_role, 'content': f'<<<{prompt_text_name}>>>: {prompt_text}'}

    if combine_messages == True:
        msg_history_string = " ".join(item["content"] for item in msg_history_list_dict_temp if item['role'] != 'system')
        reformatted_msg_history_list_dict = [{
            'role': prompt_text_role, 
            'content': msg_history_string
        }]
        reformatted_msg_history_list_dict.append(prompt_dict)
        msg_history_list_dict_temp = reformatted_msg_history_list_dict
        logger.debug(msg_history_list_dict_temp)
    else:
        msg_history_list_dict_temp.append(prompt_dict)
        logger.debug(msg_history_list_dict_temp)

    utils.write_json_to_file(
        data=msg_history_list_dict_temp, 
        variable_name_text='msg_history_list_dict_temp', 
        dirname='log/get_combine_msghistory_and_prompttext_combined', 
        include_datetime=False
    )
    return msg_history_list_dict_temp

def _count_tokens(text:str, model="gpt-3.5-turbo") -> int:
    try:
        encoding = tiktoken.encoding_for_model(model_name=model)
        tokens_in_text = len(encoding.encode(text))
    except:
        raise ValueError("tiktoken.encoding_for_model() failed")

    return tokens_in_text

def _count_tokens_in_messages(messages: List[dict]) -> int:
    try:
        total_tokens = 0
        for message in messages:
            # Using .get() with default value as an empty string
            role = message.get('role', '')
            content = message.get('content', '')

            # Count tokens in role and content
            total_tokens += _count_tokens(role) + _count_tokens(content)
        logger.info(f"Total Tokens: {total_tokens}")
        return total_tokens
    except:
        raise ValueError("_count_tokens_in_messages() failed")
def get_models(api_key=None):
    """
    Function to fetch the available models from the OpenAI API.

    Args:
        api_key (str): The API key for the OpenAI API.

    Returns:
        dict: The JSON response from the API containing the available models.
    """
    url = 'https://api.openai.com/v1/models'

    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    response = requests.get(url, headers=headers)

    return response.json()

if __name__ == '__main__':
    yaml_data = run_config()
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

    #test2 -- Get models
    gpt_models = get_models(OPENAI_API_KEY)
    # print("GPT Models:")
    # print(json.dumps(gpt_models, indent=4))

     # test3 -- call to chatgpt chatcompletion
    # openai_gpt_chatcompletion(messages_dict_gpt=[{'role':'user', 'content':'Whats a tall buildings name?'}],
    #                           max_characters=250,
    #                           max_attempts=5,
    #                           model=gpt_model,
    #                           frequency_penalty=1,
    #                           presence_penalty=1,
    #                           temperature=0.7)