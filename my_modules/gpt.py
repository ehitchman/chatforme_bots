from classes.ArticleGeneratorClass import ArticleGenerator
from classes.ConsoleColoursClass import bcolors, printc
from my_modules import utils
from my_modules.my_logging import create_logger

import os
from my_modules.config import load_env, load_yaml
import random
import requests
import openai
import tiktoken
from typing import List

import re
import json

#LOGGING
stream_logs=False
mode="gpt-3.5-turbo"

# call to chat gpt for completion TODO: Could add  limits here?
def openai_gpt_chatcompletion(messages_dict_gpt:list[dict],
                              OPENAI_API_KEY=None, 
                              max_characters=250,
                              max_attempts=5,
                              model=mode,
                              frequency_penalty=1,
                              presence_penalty=1,
                              temperature=0.7) -> str: 
    """
    Send a message to OpenAI GPT-3.5-turbo for completion and get the response.

    Parameters:
    - messages_dict_gpt (dict): Dictionary containing the message structure for the GPT prompt.
    - OPENAI_API_KEY (str): API key to authenticate with OpenAI.

    Returns:
    str: The content of the message generated by GPT.
    """  
    #Create Client
    client = create_gpt_client()   
    client.api_key = OPENAI_API_KEY

    logger_gptchatcompletion = create_logger(
        dirname='log',
        logger_name='logger_openai_gpt_chatcompletion',
        mode='a',
        stream_logs=stream_logs
        )
    logger_gptchatcompletion.debug("This is the messages_dict_gpt submitted to GPT ChatCompletion")
    logger_gptchatcompletion.debug(f"The number of tokens included is: {count_tokens_in_messages(messages=messages_dict_gpt)}")
    logger_gptchatcompletion.debug(messages_dict_gpt)

    #Error checking for token length, etc.
    counter=0
    while count_tokens_in_messages(messages=messages_dict_gpt) > 2000:
        if counter > 3:
            error_message = f"Too many tokens {token_count} even after 3 attempts to reduce count. Raising exception."
            logger_gptchatcompletion.error(error_message)
            raise ValueError(error_message)
        logger_gptchatcompletion.debug("Entered count_tokens_in_messages() > 2000")
        token_count = count_tokens_in_messages(messages=messages_dict_gpt)
        logger_gptchatcompletion.warning(f"The messages_dict_gpt contained too many tokens {(token_count)}, .pop() first dict")
        messages_dict_gpt.pop(0)
        counter+=1
    
    #Call to OpenAI
    for attempt in range(max_attempts):
        try:
            generated_response = client.chat.completions.create(
                model=model,
                messages=messages_dict_gpt,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                temperature=temperature
            )
        except Exception as e:
            logger_gptchatcompletion.error(f"Exception occurred during API call: {e}")
            logger_gptchatcompletion.error(f"Attempt {attempt + 1} of {max_attempts} failed.")
            continue
        
        logger_gptchatcompletion.debug("This is the generated response:")
        logger_gptchatcompletion.debug(generated_response)

        gpt_response_text = generated_response.choices[0].message.content
        gpt_response_text_len = len(gpt_response_text)

        logger_gptchatcompletion.debug(f"The generated_response object is of type {type(generated_response)}")
        logger_gptchatcompletion.debug(f'The --{attempt}-- call to gpt_chat_completion had a response of {gpt_response_text_len} characters')
        logger_gptchatcompletion.debug(f"The generated_response object is of type {type(gpt_response_text)}")        
        
        if gpt_response_text_len < max_characters:
            logger_gptchatcompletion.info(f'OK: The generated message was <{max_characters} characters')
            break  

        else: # Did not get a msg < n chars, try again.
            logger_gptchatcompletion.warning(f'\The generated message was >{max_characters} characters, retrying call to openai_gpt_chatcompletion')
            
            messages_dict_gpt_updated = [{'role':'user', 'content':f"Shorten this message to less than {max_characters} characters: {gpt_response_text}"}]
            generated_response = client.chat.completions.create(
                model=model,
                messages=messages_dict_gpt_updated,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                temperature=temperature
                )
            gpt_response_text = generated_response.choices[0].message.content
            gpt_response_text_len = len(gpt_response_text)

            if gpt_response_text_len > max_characters:
                logger_gptchatcompletion.warning(f'The generated message was gpt_response_text_len characters (>{max_characters}) on the second try, retrying call to openai_gpt_chatcompletion')
            elif gpt_response_text_len < max_characters:
                logger_gptchatcompletion.info(f'OK on second try: The generated message was {gpt_response_text_len} characters')
                break
    else:
        message = "Maxium GPT call retries exceeded"
        logger_gptchatcompletion.error(message)        
        raise Exception(message)

    logger_gptchatcompletion.info(f'call to gpt_chat_completion ended with gpt_response_text of {gpt_response_text_len} characters')

    return gpt_response_text

def create_gpt_client():
    client = openai.OpenAI()
    return client

def prompt_text_replacement(gpt_prompt_text,
                            replacements_dict):
    logger_prompt_text_replacement = create_logger(
        dirname='log',
        logger_name='logger_prompt_text_replacement',
        mode='a',
        stream_logs=stream_logs
        )
    prompt_text_replaced = gpt_prompt_text.format(**replacements_dict)   
    logger_prompt_text_replacement.info(f"prompt_text_replaced: {prompt_text_replaced}")
    return prompt_text_replaced

def ouat_gpt_response_cleanse(gpt_response: str) -> str:
    gpt_response_formatted = re.sub(r'<<<.*?>>>\s*:', '', gpt_response)
    return gpt_response_formatted

def chatforme_gpt_response_cleanse(gpt_response: str) -> str:
    gpt_response_formatted = re.sub(r'<<<.*?>>>\s*:', '', gpt_response)
    return gpt_response_formatted

def botthot_gpt_response_cleanse(gpt_response: str) -> str:
    gpt_response_formatted = re.sub(r'<<<.*?>>>\s*:', '', gpt_response)
    return gpt_response_formatted

def combine_msghistory_and_prompttext(prompt_text,
                                      prompt_text_role='user',
                                      prompt_text_name='unknown',
                                      msg_history_list_dict=None,
                                      combine_messages=False) -> [dict]:
    logger_msghistory_and_prompt = create_logger(
        dirname='log',
        logger_name='logger_msghistory_and_prompt',
        debug_level='DEBUG',
        mode='a',
        stream_logs=stream_logs
        )

    #deal with prompt text
    if prompt_text_role == 'system':
        prompt_dict = {'role': prompt_text_role, 'content': f'{prompt_text}'}
    if prompt_text_role in ['user', 'assistant']:
        prompt_dict = {'role': prompt_text_role, 'content': f'<<<{prompt_text_name}>>>: {prompt_text}'}

    if not msg_history_list_dict:
        msg_history_list_dict = [prompt_dict]

    # Check if msg_history_list_dict is the correct data type
    if (msg_history_list_dict is not None and not isinstance(msg_history_list_dict, list)) or \
    (msg_history_list_dict and not all(isinstance(item, dict) for item in msg_history_list_dict)):
        logger_msghistory_and_prompt.debug("msg_history_list_dict is not a list of dictionaries or None")
        raise ValueError("msg_history_list_dict should be a list of dictionaries or None")
    
    else:
        if combine_messages == True:
            msg_history_string = " ".join(item["content"] for item in msg_history_list_dict if item['role'] != 'system')
            reformatted_msg_history_list_dict = [{
                'role':prompt_text_role, 
                'content':msg_history_string
            }]            
            reformatted_msg_history_list_dict.append(prompt_dict)
            msg_history_list_dict=reformatted_msg_history_list_dict

        else:
            msg_history_list_dict.append(prompt_dict) 
            logger_msghistory_and_prompt.debug(msg_history_list_dict)

        logger_msghistory_and_prompt.debug(msg_history_list_dict)
        utils.write_json_to_file(
            data=msg_history_list_dict, 
            variable_name_text='msg_history_list_dict', 
            dirname='log/get_combine_msghistory_and_prompttext_combined', 
            include_datetime=False
            )
        return msg_history_list_dict

def _count_tokens(text:str, model="gpt-3.5-turbo") -> int:
    encoding = tiktoken.encoding_for_model(model_name=model)
    tokens_in_text = len(encoding.encode(text))
    return tokens_in_text

def count_tokens_in_messages(messages: List[dict]) -> int:
    total_tokens = 0
    for message in messages:
        role = message['role']
        content = message['content']
        total_tokens += _count_tokens(role) + _count_tokens(content)
    return total_tokens

def get_models(api_key=None):
    """
    Function to fetch the available models from the OpenAI API.

    Args:
        api_key (str): The API key for the OpenAI API.

    Returns:
        dict: The JSON response from the API containing the available models.
    """
    url = 'https://api.openai.com/v1/models'

    headers = {
        'Authorization': f'Bearer {api_key}'
    }

    response = requests.get(url, headers=headers)

    return response.json()

if __name__ == '__main__':
    yaml_data = load_yaml(yaml_dirname='config')
    load_env(env_dirname='config')
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

    # #test2 -- Get models
    # gpt_models = get_models(OPENAI_API_KEY)
    # print("GPT Models:")
    # print(json.dumps(gpt_models, indent=4))

    #test3 -- call to chatgpt chatcompletion
    # openai_gpt_chatcompletion(messages_dict_gpt=[{'role':'user', 'content':'Whats a tall buildings name?'}],
    #                           OPENAI_API_KEY=os.getenv('OPENAI_API_KEY'), 
    #                           max_characters=250,
    #                           max_attempts=5,
    #                           model=mode,
    #                           frequency_penalty=1,
    #                           presence_penalty=1,
    #                           temperature=0.7)